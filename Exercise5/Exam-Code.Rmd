---
title: "EXAM"
author: "Markus Köfler"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, results='hide',  error=FALSE, warning=FALSE, message=FALSE)
```

```{r}
#rm(list=ls())
packages <- c("tidyverse", "dplyr", "janitor", "stargazer", 
              "haven", 'bigutilsr',  'magrittr', "FinAna",
              "lubridate", "png", 'moments', 'sqldf', 
              "arules", "arulesViz",
              "factoextra", "cluster", 
              'ggdendro', 'dbscan',
              "datasets",
              "plotly", "neuralnet", "optimization", "GA", "DEoptim",
              "pso", "NMOF")

package_installer <- function (list_of_packages){
  for(package in list_of_packages) {
    if(!require(package, character.only = TRUE)) {
      install.packages(package, dependencies = TRUE)
      require(package, character.only = TRUE)
      }
    else {
      next
    }
  }
}

package_installer(packages)
```

```{r}
# create random vectors
set.seed(42)
# Generate five vectors of random numbers
vectors <- replicate(5, sample(1:100, 10, replace = TRUE))
# Print the vectors
for (i in 1:5) {
  cat("Vector", i, ":", vectors[, i], "\n")
}
v1 <- vectors[, 1]; v2 <- vectors[, 2]; v3 <- vectors[, 3]; v4 <- vectors[, 4]
# vector with outlier
v5 <- c(49, 61, 39, 52, 80, 10, 49, 42, 150, 1)
```


# 1 Outlier Analysis and Vizz
vectors with outliers:
`Vector 1 : 49 65 25 74 100 18 49 47 100 1

Vector 2 : 100 89 37 20 26 3 41 89 1 100 

Vector 3 : 95 5 84 34 92 3 58 97 100 100 

Vector 4 : 30 43 15 22 58 8 36 68 100 100 

Vector 5 : 92 69 4 98 50 99 88 87 1 1`

- boxplots()
Outliers are usually the points which fall 1.5 times the IQ-range

```{r, results ='markup'}
quantile(sort(v5))

quantile(sort(v5), 0.75)

IQR(sort(v5))
```

```{r, results='hide'}
data("USJudgeRatings")
USJR <-  USJudgeRatings
# to create boxplots: boxplot(array ...)
par(mfrow=c(3,4))
 # i = 1, 2, 3, ..., number of colums
for (i in 1:length(colnames(USJR))) {
  boxplot(USJR[, i], main= as.character(colnames(USJR)[i]),col=12, alpha=0.5, plot=F)
}

```

```{r}
outliers <- boxplot(v5, plot=FALSE)$out
# check if outliers are in the vector
outliers %in% v5
# get the indices
which(outliers == v5)
```



-   z-score 
$$
    z = \frac{x - \bar{x}}{\sigma} = \frac{x-\mu}{\sigma}
$$

```{r}
z <- (v1 - mean(v1)) / sd(v1)
abs(z)  > 1.8 # outlier threshold
mask <- abs(z)  > 1.8

z[mask] # the outlier identified

outlier_df <- ifelse(abs(z) >= 1.8, 1, 0) %>% as.data.frame()
```


- Outlier clustering with `DBSCAN`
```{r}
library(dbscan)
dist_mtx <- dist(USJR, method = "euclidean") # euclidean distance
# perform DBSCAN clustering
cluster <- dbscan(dist_mtx, eps = 1, minPts = 5)

# print the outlier indices
outlier_indices <- which(cluster$cluster == 0)
print(outlier_indices)
```

- Skewness & Kurtosis
```{r}
data("USJudgeRatings")
USJR <-  USJudgeRatings
# initiating empty vectors to store values
s <- c()
k <- c()
for (i in 1:length(colnames(USJR))) {
  skew <- skewness(USJR[, i]); s <- append(s, skew);
  kurt <- kurtosis(USJR[, i]); k <- append(k, kurt);
}
USJR <- USJR %>% rbind(s, k)
n <- nrow(USJR)
rownames(USJR)[c(n-1, n)] <- c("skewness", "kurtosis")
USJR[c(n-1, n), ]
```













# 2 Associate Rule Learning
```{r}
retail <- read.csv(
  "https://raw.githubusercontent.com/MarkusStefan/Data_Analytics/main/Exercise2/retail2.csv?token=GHSAT0AAAAAACBJAQ7CZWQDFSYIQT3UXJVGZBV267Q", 
  sep=';')

v1 <- c(1, 0, 0, 1, 1, 0, 1, 0, 1, 0)
v2 <- c(0, 0, 1, 1, 0, 1, 1, 1, 0, 0)
v3 <- c(1, 1, 0, 0, 1, 1, 0, 1, 0, 1)
v4 <- c(0, 1, 0, 1, 1, 1, 0, 0, 0, 1)
```

**Support**: It indicates how frequently an item set appears in the data set.
With $frq(A)$ as the number of occurrences from the total number of
transactions $frq(T) = T$:
$$
supp(A) = \frac{frq(A)}{T}
$$ For one single set

```{r}
support <- function(A, B=c()){
  T. <- length(A)
  if (length(B) == length(c())){ # check if  is empty
    #frq_x <- sum(x)
    mask <- A == 1
    freqA <- length(A[mask])
    suppA <- freqA/T.
    return(suppA)
  } 
  
    mask <-  A == 1 & B == 1
    freqAB <- length(A[mask])
    suppAB <- freqAB/T.
    return(suppAB)
   
  
}

support(A=v1, B=v2)
# identical to:
sum(v1==1 & v2==1)/length(v1)
```





**Confidence**: It says how likely item set B is purchased when item set
A is purchased

$$
conf(A \to B) = \frac{supp(A,B)}{supp(A)}
$$
with $supp(A,B) = \frac{frq(A,B)}{frq(T)}$ and
$supp(A) = \frac{frq(A)}{frq(T)}$ it holds that:

$$
conf(A \to B) = \frac{supp(A,B)}{supp(A)} = 
\frac{\frac{frq(A,B)}{frq(T)}}{\frac{frq(A)}{frq(T)}} = 
\frac{frq(A,B)}{frq(A)}
$$

```{r}
confidence <- function(A, B){
  suppAB <- support(A, B)
  suppA <- support(A)
  confAB <- suppAB/suppA
  return(confAB)
}

confidence(v1, v2)
```







**Lift**: It says how likely item set B is purchased when item set A is
purchased while controlling for how popular item set B is.

Lift is the ratio of the observed support to that expected if A and B
were independent or equivalently the ratio of the confidence of the rule
to the expected confidence of the RHS item set by independence.

$$
lift(A \to B) = \frac{conf(A,B)}{supp(B)} = \frac{supp(A,B)}{supp(A) \times supp(B)}
$$

*Note:*

$$
lift(A \to B) == lift(B \to A)
$$

```{r}
lift <- function(A, B){
  confAB <- confidence(A, B)
  suppB <- support(B)
  liftAB <- confAB / suppB
  return(liftAB)
}

lift(v1, v2)
```




- Apriori

```{r}
library(arules)
library(datasets)
# ? apriori
data("Groceries")

Groceries_rules <- apriori(Groceries, 
                           parameter = list(support = 0.01, #list(support = 0.005, 
                                            confidence = 0.4)) #confidence = 0.3
# top 3 ruels with highest support
inspect(sort(Groceries_rules, by = "support", 
             decreasing = TRUE)[1:3])
# top 3 rules with highest confidence
inspect(sort(Groceries_rules, by = "confidence", 
             decreasing = TRUE)[1:3])
# top 3 rules with highest lift
inspect(sort(Groceries_rules, by = "lift", 
             decreasing = TRUE)[1:3])
```



# 3 Clustering

**Euclidean Distance for multiple dimensions:**

$$
D_{A,B} = \sqrt{
(A1 − B1)^2 + (A2 − B2)^2 + \dots + (An − Bn)^2
}
$$
**Weighted Euclidean Distance:**

$$
D_{A,B} = \sqrt{\alpha_1(A1 − B1)^2 + \alpha_2(A2 − B2)^2 + \dots + \alpha_n(An − Bn)}
$$

```{r}
euclidean <- function(A, B, weights=c()){
  
  if (length(weights) == length(c())){
    ed <- as.numeric(sqrt(sum((A-B)**2)))
    return(ed)
  }
  edw <-  as.numeric(sqrt(sum( ((A-B)*weights)**2 )))
  return(edw)
}
euclidean(v1, v3, weights=c(1,1,1,1,0.5,1,1,1,1, 5))
# dist only computes single distances for 2 data points, hence the matrix
#d <- as.matrix(dist(dataframe_or_matrix, method="euclidean"))
#d
# identify the states with the highest/lowest euclidean distance
# arrests[order(arrests$euclids, decreasing = F), ] %>% head(n=1)

```
- Hierarchical clustering
```{r, include=TRUE, eval=FALSE}
dist_data<-dist(scale(USArrests, center = TRUE, scale = TRUE))
dist_data_unscaled <-dist(USArrests)
hclust_cmplt_scaled <- hclust(dist_data, method = 'complete') # "minowski"
hclust_cmplt <- hclust(dist_data_unscaled, method = 'complete')
# plot(hclust_cmplt_scaled) # plots a dendrogram
```

- K-Means
```{r, include=TRUE, eval=FALSE}
data("USArrests")
set.seed(123) 
k <- 2
km <- kmeans(USArrests, centers = k, nstart = 41)
km$cluster
km$center
# tag each state according to their cluster
clust_labs <- ifelse(km$cluster == 1, "high crime",  "~low crime")
USArrests_clust <- data.frame(USArrests, cluster = clust_labs)
USArrests_clust
# Print the number of states in each cluster
table(USArrests_clust$cluster)
cat('\n')
Ohio_clust <- USArrests_clust[which(row.names(USArrests_clust) == 
                                    "Ohio"), "cluster"]
USArrests_clust[which(rownames(USArrests_clust)=="Ohio"), ]
A$cluster[iloc_ohio]
cat("\nOhio is in cluster:\t", A$cluster[iloc_ohio]) 
#fviz_cluster(A, data = USArrests)
```
- DBSCAN
epsilon neighborhood (threshold density) and minimum points in the epsilon region
```{r, include=T, eval=F}
data(USArrests)
USArrests <- data.frame(scale(USArrests))

db1 <- dbscan(USArrests, eps = 0.5, MinPts = 3) 
#fviz_cluster(db1, USArrests, geom = c("point","text"), labelsize=8, 
#             xlab=colnames(USArrests)[1], ylab=colnames(USArrests)[2]) + 
#  labs(subtitle = 'epsilon = 0.5, min.points = 3')
```






# 4 Regression Analysis
Use t test and check if temperature is significantly different from 79.

$H_0:\quad \text{The temperature is equal to 79}$

$H_1:\quad \text{The temperature is not equal to 79}$

$$
t = \frac{\hat{\beta_j}}{\hat{se(\hat{\beta_j}})}
$$

$$
t = \frac{\rho_{x,y}}{\sqrt{1-\rho_{x,y}}}\times \sqrt{n-2}
$$

the critical value is:

$$
t = \frac{\hat{\beta_1}+\hat{\beta_2}}{\hat{se}(\hat{\beta_1}+\hat{\beta_2})}
$$

wherby

$$
\hat{se}(\hat{\beta_1}+\hat{\beta_2}) = \sqrt{\hat{se}(\hat{\beta_1}) + \hat{se}(\hat{\beta_2}) + 2* \hat{cov}(\hat{\beta_1},\hat{\beta_1})}
$$

df = (n-k-1) k-number of independent variables -1 \|\| (n-p) p-number of
coefficients

```{r}
data("airquality")
air <- airquality
# test whether the mean of Temp significantly differs from 79
t.test(air$Temp, mu = 79)
```

79 is included in the CI, therefore the $H_0$ is not rejected! Also, the
p-value 14.62% threshold is higher than the common significance level of
5%, so there is NOT enough evidence to reject the null.


Split the data into sample A with observations 1 to 77 and sample B with
observations 78 to 153. Use t test and check if temperature is
significantly different between samples A and B.

$H_0:\quad \text{The temperature is equal in both samples A and B}$

$H_1:\quad \text{The temperature is not equal in both samples A and B}$


```{r}

A <- air[1:77,]
B <- air[78:153,]

# two-sample t-test
t.test(A$Temp, B$Temp)
```

The p-value threshold is much lower than 5% ---\> 0.0000211 \< 0.05.
Therefore the $H_0$ is rejected at the 5% significance level. This
suggests that the temperature differs significantly in both samples!



Calculate the parameters of the linear regressions ($\beta_0$ and
$\beta_1$) according to equations (2) and (3) from chapter 5.

$$
(1)\qquad \hat{\beta}_0 = \bar{y} - \hat{\beta}_1\bar{x}
$$
$$
(2)\qquad \hat{\beta}_1 = \frac{\sum_{t=1}^T (x_t-\bar{x}) (y_t-\bar{y})}{
\sum_{t=1}^T (x_t-\bar{x})^2}
$$


$$
R^2 = corr(\hat{y}, y)^2 = \frac{ESS}{TSS} = 
1- \frac{RSS}{TSS}
$$

$$
Adj. R^2 = 1-\frac{RSS}{TSS}\times \frac{T-1}{T-N}
$$

with $RSS = \sum_{t=1}^T(y_t-\hat{y}_t)^2$ ,
$TSS = \sum_{t=1}^T(y_t-\bar{y}_t)^2$ and
$ESS = \sum_{t=1}^T(\hat{y}_t-\bar{y}_t)^2$

whereby TSS = RSS + ESS, or equally:

$$
Adj. R^2 = 1- \frac{(1-R^2)(T-1)}{T-p-1}
$$

with $T$ as sample size, $p$ as number of predictors.





# 5 Heuristics & Neural Networks

Consider the following objective function: 
$$
f(x, y) = x^2 y − 2xy^2 + 3xy + 20.
$$

Run a grid search with 100 grid points in each dimension to minimize the given function and to identify the minimum. $x \in [−2, 2], y \in [−2, 2]$.

```{r}
x <- seq(-2, 2, length.out = 100)
y <- seq(-2, 2, length.out = 100)
mat <- matrix(NA, nrow = 100, ncol = 100)
for (i in 1:100) {
  for (k in 1:100){
    mat[i,k] <- x[i]^2 * y[k] - 2 * x[i] * y[k]^2 + 3 * x[i] * y[k] + 20
  }
}
min(mat)
# x is col y is row.
which(mat == min(mat), arr.ind = TRUE)
y[100]
x[1]
#plot_ly(x=x,y=y,z = ~mat, type = "surface")
```

- DEOptim
```{r, include=T, eval=F}
library("DEoptim")

f <- function(x){
  sum((eu$DAX-x[1]-x[2]*eu$SMI-x[3]*eu$CAC-x[4]*eu$FTSE)^2)
}


ff <- DEoptim(f, lower = c(-200, -1, -1, -1), upper = c(0, 3, 3, 3), control = list(itermax = 200, trace = 20))

best <- ff$optim$bestmem
best

bad <- sum((eu$DAX - best[1] - best[2] * eu$SMI - best[3] * eu$CAC - best[4] * eu$FTSE)^2)
good <- sum(ols$residuals^2)
bad
good
```

- Particle Swarm Optimization
```{r, include=T, eval=F}
library("pso")
data("EuStockMarkets")
eu <- data.frame(EuStockMarkets)
f <- function(x){
  sum((eu$DAX -  x[1] - x[2]*eu$SMI - 2*x[3]*eu$CAC - x[4]*eu$FTSE) ^2)
}
DEoptim(f, lower = c(-200, -1, -1, -1), upper = c(0, 3, 3, 3), control = list(itermax = 200, trace = 20))
psoptim(rep(NA, 4), f, lower = c(-200, -1, -1, -1), upper = c(0, 3, 3, 3), control = list(maxit = 200, trace = 1, REPORT = 20))
```


- K-Means vs. Neural Network
```{r, include=T, eval=F}
set.seed(2009)
library("datarium")
data("marketing")
scaled <- data.frame(scale(marketing))
B <- kmeans(scaled, 2, nstart = 100)
B$cluster
scaled[5] <- B$cluster
colnames(scaled)[5] <- "cluster"
#fviz_cluster(B, data = marketing)

train <- scaled[1:(0.90*nrow(scaled)), ]
test <- scaled[(0.901*nrow(scaled)):201, ]

library("neuralnet")
nn <- neuralnet(cluster == 2 ~ youtube + facebook + newspaper + sales, train, linear.output = FALSE)
result <- predict(nn, test)
result[result > 0.95] <- 2
result[result <  0.05] <- 1
scaled$cluster[180:200]
result-scaled$cluster[180:200]
```




- DecisionTreeClassifier & RandomForrest
```{r}
#train <- scaled[1:(0.90*200),]
#test <- scaled[(0.901*200):201,]
library("party")
#f <- partykit::ctree(cluster ~ life_expect + gdp + fertility + mobile_phones + migration, data = train)
#plot(f)
library("rpart")
#ff<- rpart(cluster ~ life_expect + gdp + fertility + mobile_phones + migration, data = training, method = 'class')
#rpart.plot::rpart.plot(ff)
#predict(ff, test)[,1]
#scaled$cluster[(0.901*nrow(scaled)):nrow(scaled)]
#prediction$predict <- predict(ff, test)[,1]
```

