---
title: "Cluster_Analysis"
author: "Markus Köfler"
date: "`r Sys.Date()`"
output: html_document
editor_options: 
  markdown: 
    wrap: 72
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r include=FALSE}
rm(list=ls())
packages <- c("tidyverse", "dplyr", "janitor", "stargazer", 
              "haven", 'bigutilsr',  'magrittr', "FinAna",
              "lubridate", "png", "factoextra", "cluster", 
              'ggdendro', 'dbscan')

package_installer <- function (list_of_packages){
  for(package in list_of_packages) {
    if(!require(package, character.only = TRUE)) {
      install.packages(package, dependencies = TRUE)
      require(package, character.only = TRUE)
      }
    else {
      next
    }
  }
}

package_installer(packages)
```

# 3.1) Distances

## (a)

Use the "USArrests" data as discussed in the lecture. Calculate the
euclidean distances between states using '`for`-loops'.

**Euclidean Distance for multiple dimensions:**

$$
D_{A,B} = \sqrt{
(A1 − B1)^2 + (A2 − B2)^2 + \dots + (An − Bn)^2
}
$$

```{r}
data("USArrests")
#show(USArrests)
#n <- nrow(USArrests)
#comb_states<-combn(1:n, 2, simplify = FALSE)
#comb_states
USArrests[1:5, ]
# creating a csv file
#write.csv(USArrests, file='USArrests.csv')
```

```{r eval=FALSE, include=FALSE}
states <- c()
euclids <- c()
for (i in 1:(nrow(USArrests)-1)){
  A <- USArrests[i, ]
  B <- USArrests[i+1, ]
  #print(A); print(B)
  euclid <- (A-B)**2 %>% sum() %>% sqrt() %>% as.numeric()
  states <- states %>% append(paste0(rownames(A),'-', rownames(B)))
  euclids <- euclids %>% append(euclid)
  
  
  
  
}
#states;euclids
arrests <- data.frame(states, euclids)
arrests
```

```{r}
states <- c()
euclids <- c()
for (i in 1:(nrow(USArrests)-1)){
  n <- i+1
  # nested loop
  while (n <= (nrow(USArrests))){
    A <- USArrests[i, ]
    B <- USArrests[n, ]
    euclid <- (A-B)**2 %>% sum() %>% sqrt() %>% as.numeric()
    states <- states %>% append(paste0(rownames(A),' - ', rownames(B)))
    euclids <- euclids %>% append(euclid)
    n <- n+1
  }
}

arrests <- data.frame(states, euclids)

# outputs the 5 state-combinations with the lowest eurclidean distance
arrests[order(arrests$euclids, decreasing = F), ] %>% head(n=5)
```

```{r}
euclid_mtx <- dist(USArrests, method = "euclidean") %>% as.matrix()
par(mar = c(5, 6, 4, 2) + 0.1, cex.lab = 1.5, cex.axis = 1.2)
heatmap(euclid_mtx,
        main='Euclidean Distance among US-States')
legend("topright",
       legend = c("High Distance", "Low Distance"),
       fill = c("darkred", "beige"),
       bty = "o")
```

## (b)

Identify two states with the maximal/minimal distance.

```{r}
arrests[order(arrests$euclids, decreasing = F), ] %>% head(n=1) 
arrests[order(arrests$euclids, decreasing = F), ] %>% tail(n=1)
```

## (c)

Calculate the weighted euclidean distances between states (scaled data;
the weight of 'UrbanPop' should be 0.25; all other weights should be 1).

**Weighted Euclidean Distance:**

$$
D_{A,B} = \sqrt{\alpha_1(A1 − B1)^2 + \alpha_2(A2 − B2)^2 + \dots + \alpha_n(An − Bn)}
$$

To scale the data (assign weights)

```{r eval=FALSE, include=FALSE}
help(dist)
help(scale)
```

```{r}
# create a vector of weights
weights <- c(1, 1, 0.25, 1)

states <- c()
euclids <- c()

for (i in 1:(nrow(USArrests)-1)){
  n <- i+1
  while (n <= (nrow(USArrests))){
    A <- USArrests[i, ]
    B <- USArrests[n, ]
    
    # calculate the weighted euclidean distance
    diff <- (A - B) * weights
    euclid <- sqrt(sum(diff^2))
    
    # append the results to the output vectors
    states <- states %>% append(paste0(rownames(A),' - ', rownames(B)))
    euclids <- euclids %>% append(euclid)
    
    n <- n+1
  }
}

arrests_weighted <- data.frame(states, euclids)

```

```{r}
# state combination with lowest weighted eucledian distance
arrests_weighted[order(arrests_weighted$euclids, decreasing = F),][1,]

# state combination with highest weighted eucledian distance
arrests_weighted[order(arrests_weighted$euclids, decreasing = F),][nrow(arrests_weighted),]
```

lowest euclidean distance remain the same: Iowa - New Hampshire

Highest eucledian distance is now different: North Carolina - North
Dakota (without weights: Florida - North Dakota

# (3.2) Hierarchical clustering

*Note*: Implementation in Python (Google Colab Notebook also on GitHub)

## (a)

Run hierarchical clustering analysis with scaled "USArrests" data. Use
the 'complete' method as linkage function. Plot a dendrogram and argue
how many clusters you would choose.

Note: Regarding the scale function which defaults to
`scale(x, center=TRUE, scale=TRUE)`

-   If `center` is `TRUE` then centering is done by subtracting the
    column means

-   If `scale` is `TRUE` then scaling is done by dividing the (centered)
    columns of `x` by their standard deviations if `center` is `TRUE`,
    and the root mean square otherwise

This implies that each row is standardized/normalized (like z-score),
that is, subtracting the mean of each column's value and dividing the
standard deviation. Thereby, the features of the data is preserved,
however, it is compressed into a a smaller range of values, making
computations much more efficient especially on huge data sets

$$
\tilde{x} = \frac{x_i-\bar{x}}{\sigma}
$$

As a result, the mean of the series $\tilde{x}$ will be 0 and the
standard deviation will be 1 (normal distribution
$\tilde{x} \sim \mathcal{N}$)

```{r}
dist_data<-dist(scale(USArrests))
dist_data_unscaled <-dist(USArrests)
hclust_cmplt_scaled <- hclust(dist_data, method = 'complete') 
hclust_cmplt <- hclust(dist_data_unscaled, method = 'complete')


dendro <- ggdendrogram(hclust_cmplt_scaled)
dendro + 
  theme(panel.grid.major.y = element_line(color = "black", size = 0.3)) +
  ggtitle('Method: Complete') +
  geom_hline(yintercept = 3.5, col=2, lwd=1, lty=2)

#dist_data<-dist(scale(USArrests))
#hclust_cmplt_scaled <- hclust(dist_data, method = 'complete') 
plot(hclust_cmplt_scaled, main = 'Dendrogram scaled data') #dendrogram
plot(hclust_cmplt, main='Dendrogram unscaled data') #dendrogram
```

In this example, I would choose **4** clusters. As the dendrogram
suggests, the difference is only quite large for 4 individual paths, the
distance among the smaller potential clusters is quite small.

```{r eval=FALSE, include=FALSE}

k <- 1
while (k < 10){
    k_clusters <- cutree(hclust_cmplt, k = 15)
    silhouette(k_clusters, dist_data) %>% plot()
    k <- k+1
}
```

```{r eval=FALSE, include=FALSE}
scaled_data <- scale(USArrests)

# Perform hierarchical clustering
hc_complete <- hclust(dist(scaled_data), method = "complete")

# Plot dendrogram
plot(hc_complete, main = "Dendrogram of USArrests Data (Complete Linkage)", xlab = "State", ylab = "Distance")
# Cut dendrogram into 2 clusters
k2_clusters <- cutree(hc_complete, k = 4)

# Plot silhouette plot for k = 2 -> library(cluster)
silhouette(k2_clusters, dist(scaled_data)) %>% plot(main='')
```

## (b)

Run hierarchical clustering analysis with scaled "USArrests" data. Use
the 'single' method as linkage function. Plot a dendrogram and argue how
many clusters you would choose.

```{r}
hclust_sngl <- hclust(dist_data, method = 'single')
dendro <- ggdendrogram(hclust_sngl)

dendro + 
  theme(panel.grid.major.y = element_line(color = "black", size = 0.3)) +
  ggtitle('Method: Single') +
  geom_hline(yintercept = 1.5, col=2, lwd=1, lty=2)


plot(hclust_sngl)

k_clust <- cutree(hclust_sngl, k = 2)
silhouette(k_clust, dist(scaled_data)) %>% plot(main='')
```

## (c)

Run hierarchical clustering analysis with scaled "USArrests" data with
Minkowski distance measure (p=3). Use the 'complete' method as linkage
function. Plot a dendrogram and argue how many clusters you would
choose.

```{r}
# scale the data
scaled_data <- scale(USArrests)

# hierarchical clustering with Minkowski distance measure
hc_minkowski <- hclust(dist(scaled_data, method = "minkowski", p = 3), method = "complete")
dendro <- ggdendrogram(hc_minkowski)

dendro + 
  theme(panel.grid.major.y = element_line(color = "black", size = 0.3)) +
  ggtitle('Method: Minowsky', subtitle = 'p=3') +
  geom_hline(yintercept = 2.85, col=2, lwd=1, lty=2)

# Plot dendrogram
plot(hc_minkowski, main = "Dendrogram of USArrests Data (Minkowski, p=3)", xlab = "State", ylab = "Distance")

k2_clusters <- cutree(hc_minkowski, k = 5)

silhouette(k2_clusters, dist(scaled_data, method = "minkowski", p = 3)) %>% plot(main='')
```

# (3.3) k-means clustering on scaled "USArrests" data

## (a)

Run k-means clustering analysis with 2 clusters. Which cluster would you
denote as potentially high/low crime cluster? Is Ohio high or low crime
state?

```{r}
data(USArrests)
# determining index location of Ohio
iloc_ohio <- which(rownames(USArrests)=="Ohio")
# scale the data
#USArrests <- data.frame(scale(USArrests))
# scaling the data
arrests_sc <- scale(USArrests, center=T, scale=T) %>% dist()

km <- kmeans(arrests_sc, centers = 2, nstart = 10)
km$cluster[iloc_ohio]
```

```{r}
# Load the USArrests dataset
data("USArrests")

# k-means clustering with 2 clusters
set.seed(123) # for reproducibility
k <- 2
km <- kmeans(USArrests, centers = k, nstart = 41)

# tag each state according to their cluster
clust_labs <- ifelse(km$cluster == 1, "high crime",  "~low crime")
USArrests_clust <- data.frame(USArrests, cluster = clust_labs)
USArrests_clust
# Print the number of states in each cluster
table(USArrests_clust$cluster)
Ohio_clust <- USArrests_clust[which(row.names(USArrests_clust) == "Ohio"), "cluster"]
Ohio_clust
```

```{r}
data(USArrests)
iloc_ohio <- which(rownames(USArrests)=="Ohio")
USArrests <- data.frame(scale(USArrests))



set.seed(200996)
A <- kmeans(USArrests, 2, nstart = 100)
A$centers
A$cluster[iloc_ohio]


B <- kmeans(USArrests, 4, nstart = 100)
B$centers
B$cluster[iloc_ohio]

arrests3dim <- USArrests %>% select(-UrbanPop)
C <- kmeans(USA, 3, nstart = 100)
C$centers
C$cluster[iloc_ohio]

fviz_cluster(A, data = USArrests)
fviz_cluster(B, data = USArrests)
fviz_cluster(C, data = USArrests)
#? fviz_cluster
```

## (b)

Run k-means clustering analysis with 4 clusters. How can you
characterize these clusters? In which cluster is Ohio?

```{r}

```

## (c)

Remove the item 'UrbanPop' from the data set. Run k-means clustering
analysis with 3 clusters. How can you characterize these clusters? In
which cluster is Ohio?

# (3.4) DBSCAN clustering on scaled "USArrests" data

## (a)

Run DBSCAN clustering analysis with size of the epsilon neighborhood
(threshold density) equal 0.5 (i.e. eps = 0.5) and number of minimum
points in the eps region equal 3. How many clusters can you identify? In
which cluster is Ohio?

```{r message=FALSE, warning=FALSE}
# suppress warnings
options(warn = 2)

# load data afresh
data(USArrests)
USArrests <- data.frame(scale(USArrests))

db1 <- dbscan(USArrests, eps = 0.5, MinPts = 3)

fviz_cluster(db1, USArrests, geom = c("point","text"), labelsize=8, 
             xlab=colnames(USArrests)[1], ylab=colnames(USArrests)[2]) + 
  labs(subtitle = 'epsilon = 0.5, min.points = 3')

fviz_cluster(db1, USArrests, geom = c("point","text"), labelsize=8, axes=c(1,4), 
             xlab=colnames(USArrests)[1], ylab=colnames(USArrests)[4]) + 
  labs(subtitle = 'epsilon = 0.5, min.points = 3')

# since the order of the states is preserved we can label the cluster 
# sequences by their corresponding state and eventually find Ohio
names(db1$cluster) <- USArrests %>% rownames()
cat('Ohio is in cluster', db1$cluster[which(names(db1$cluster)=="Ohio")])
```

## (b)

Run DBSCAN clustering analysis with size of the epsilon neighborhood
(threshold density) equal 1 (i.e. eps = 1) and number of minimum points
in the eps region equal 3. How many clusters can you identify? In which
cluster is Ohio?

```{r message=FALSE, warning=FALSE}
db2 <- dbscan(USArrests, eps = 1, MinPts = 3)
fviz_cluster(db2, USArrests, geom = c("point","text"), labelsize=8, 
             xlab=colnames(USArrests)[1], ylab=colnames(USArrests)[2]) + 
  labs(subtitle="epsilon = 1, min.points = 3")
fviz_cluster(db2, USArrests, geom = c("point","text"), labelsize=8, axes=c(1,4), 
             xlab=colnames(USArrests)[1], ylab=colnames(USArrests)[4]) + 
  labs(subtitle = 'epsilon = 0.5, min.points = 3')

# since the order of the states is preserved we can label the cluster 
# sequences by their corresponding state and eventually find Ohio
names(db2$cluster) <- USArrests %>% rownames()
cat('Ohio is in cluster', db2$cluster[which(names(db1$cluster)=="Ohio")])
```

## (c)

Run DBSCAN clustering analysis with size of the epsilon neighborhood
(threshold density) equal 1.5 (i.e. eps = 1.5) and number of minimum
points in the eps region equal 3. How many clusters can you identify? In
which cluster is Ohio?

```{r message=FALSE, warning=FALSE}
db3 <- dbscan(USArrests, eps = 1.5, MinPts = 3)
fviz_cluster(db3, USArrests, geom = c("point","text"), labelsize=8, 
             xlab=colnames(USArrests)[1], ylab=colnames(USArrests)[2]) +
  labs(subtitle = "epsilon = 1.5, min.points = 3")

names(db3$cluster) <- USArrests %>% rownames()
cat('Ohio is in cluster', db3$cluster[which(names(db1$cluster)=="Ohio")])
```

# (3.5) Clustering analysis of the countries

| Variable                                              | Explanation                                                                                     |
|-------------------------------------------------------|-------------------------------------------------------------------------------------------------|
| country                                               | self explanatory                                                                                |
| population                                            | total population of respective country                                                          |
| life_expect                                           | life expect. at birth assuming const. patterns of mortality the entire life (no extraord. risk) |
| fertility                                             | children per woman within "child bearing age"                                                   |
| fertility_adol                                        | children per 1000 women aged 15-19                                                              |
| mortal_5                                              | children dying before turning 5                                                                 |
| mobile_phones                                         | communication tech infrastructure (phones, internet)                                            |
| migration                                             | net migration (immigration - emigration)                                                        |
| electricity                                           | \% of pop having access to electricity                                                          |
| gdp (per capita), inflation (CPI), pop growth, unrate | self explanatory                                                                                |

## (a)

Use the socioeconomic profiles of countries in 2020 (main source:
Worldbank) and check the considered indicators.

```{r}
se <- read.csv('https://raw.githubusercontent.com/MarkusStefan/Data_Analytics/main/Exercise3/countries_data1.csv')
exp <- readxl::read_xlsx("/Users/markuskofler/OneDrive - Alpen-Adria Universität Klagenfurt/R/DataAnalytics/Ex3/countries_indicators.xlsx")[1:2]
se %>% colnames()
exp
se %>% head()
```

## (b)

Run hierarchical clustering analysis of the countries.

```{r}
# setting country as index as functions only work with numerical datase
se_ <- se
rownames(se_) <- se$country
tryCatch( 
  expr = {
    se_ <- se_ %>% select(-country)
  },

  finally = {
    dist <- se_ %>% scale() %>% dist() 

    hc <- hclust(dist, method = "ward.D2")
    
    ggdendrogram(hc) +
      theme(panel.grid.major.y = element_line(color = "gray70", size = 0.3),
            panel.grid.minor.y = element_blank())
  }
)
dist <- se_ %>% scale() %>% dist() 
hc <- hclust(dist, method = "ward.D2")
    
ggdendrogram(hc) +
  theme(panel.grid.major.y = element_line(color = "gray70", size = 0.05),
            panel.grid.minor.y = element_blank()) +
  geom_hline(yintercept = 18.5, col='red', lty=2)
```

I would choose 3 clusters using the HC method.

## (c)

Run k-means clustering analysis of the countries.

```{r}
set.seed(45)
scaled <- scale(se_, center=T, scale=T)

km <- kmeans(scaled, 3, nstart = 100, algorithm = "Hartigan-Wong")
#km$centers
#km$cluster

which(names(km$cluster)== "Austria") # 8
km$cluster[8]

#which(km$cluster==2) %>% names()
#which(km$cluster == 3) %>% names()
#which(km$cluster == 1) %>% names()

fviz_cluster(km, data=se_, axes=c(1,2), repel=T, xlab=colnames(se_)[1], ylab=colnames(se_)[2])
```

Also with K-Means, 3 Clusters seem to be the best choice.
